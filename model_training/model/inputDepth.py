import torch
import torch.nn as nn

# https://chat.qwen.ai/c/d5478f80-9194-44b0-a5c5-d4cb9e9a59d4
# patentscope.wipo.int/search/en/WO2025128464

# From patent section [00033]:
# "a frame model can autoregressively generate later frame tokens based on input frame tokens representing
#  output frames created by the output depth model, which in some instances may be similar to or different from a first frame token
# generated by the frame model and associated with the same frame."

# From [00062]:
# "two or more of the input depth model(s), the frame model, and the output depth model(s) can be jointly trained to facilitate the operation of multiple models on similar (e.g. same) embeddings."

# From the patent's experimental results in [00074-00075], this closed-loop approach achieved:
# Lower word error rates (better speech recognition accuracy)
# Higher audio quality scores
# Better temporal coherence compared to systems without this feedback loop


class InputDepthModel(nn.Module):
    """
    Input Depth Model: Compresses generated RVQ tokens back into frame tokens.

    Based on patent sections [00033], [00059]:
    - Takes generated RVQ tokens and creates input frame token
    - Enables closed-loop training with Frame Model
    - Can use bidirectional attention since it's encoding, not generating
    """

    def __init__(
        self,
        rvq_levels: int = 12,
        embedding_dim: int = 512,
        num_layers: int = 3,
        num_heads: int = 8,
        codebook_size: int = 1024,
    ):
        super().__init__()
        self.rvq_levels = rvq_levels
        self.embedding_dim = embedding_dim

        # Embedding layers for RVQ tokens
        self.rvq_embeddings = nn.ModuleList(
            [nn.Embedding(codebook_size, embedding_dim) for _ in range(rvq_levels)]
        )

        # Bidirectional transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embedding_dim,
            nhead=num_heads,
            dim_feedforward=embedding_dim * 4,
            batch_first=True,
            dropout=0.1,
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer, num_layers=num_layers, norm=nn.LayerNorm(embedding_dim)
        )

        # Pooling layer to get single frame token
        self.pooling = nn.Sequential(
            nn.Linear(embedding_dim, embedding_dim),
            nn.GELU(),
            nn.Linear(embedding_dim, embedding_dim),
        )

    def forward(self, rvq_tokens: torch.Tensor) -> torch.Tensor:
        """
        Args:
            rvq_tokens: (batch_size, rvq_levels)
                RVQ tokens generated by Depth Model
w
        Returns:
            input_frame_token: (batch_size, embedding_dim)
                Compressed frame token representation
        """
        batch_size, rvq_levels = rvq_tokens.shape

        # Embed each RVQ level
        embedded_tokens = []
        for level in range(rvq_levels):
            level_tokens = rvq_tokens[:, level]  # (batch_size,)
            embedded = self.rvq_embeddings[level](
                level_tokens
            )  # (batch_size, embedding_dim)
            embedded_tokens.append(embedded)

        # Stack embeddings to form sequence
        token_sequence = torch.stack(
            embedded_tokens, dim=1
        )  # (batch_size, rvq_levels, embedding_dim)

        # Apply transformer encoder
        transformer_output = self.transformer(
            token_sequence
        )  # (batch_size, rvq_levels, embedding_dim)

        # Average pool across RVQ levels
        pooled = transformer_output.mean(dim=1)  # (batch_size, embedding_dim)

        # Final projection
        input_frame_token = self.pooling(pooled)  # (batch_size, embedding_dim)

        return input_frame_token


if __name__ == "__main__":
    from torchinfo import summary
    
    # Create model instance with typical parameters
    model = InputDepthModel(
        rvq_levels=12,
        embedding_dim=512,
        num_layers=3,
        num_heads=8,
        codebook_size=1024
    )
    
    print("InputDepthModel Architecture:")
    print(model)
    print(f"\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # Test forward pass
    import torch
    rvq_tokens = torch.randint(0, 1024, (1, 12))  # (batch_size=1, rvq_levels=12)
    output = model(rvq_tokens)
    print(f"\nForward pass test - Input: {rvq_tokens.shape}, Output: {output.shape}")
    
    # Try torchsummary (may not work with complex models)
    print("\nTrying torchsummary...")
    # Create a proper dummy input with correct dtype
    dummy_input = torch.randint(0, 1024, (1, 12), dtype=torch.long)

    # Now use input_data instead of input_size
    summary(model, input_data=dummy_input, device='cpu')
